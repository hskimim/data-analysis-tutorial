{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- 예측(prediction)에서 최종 목표는 실제 출력값, 즉 타겟(target)과 가장 approximate한 값을 출력하는 model을 찾는 것이다.(ex) 가중치 벡터(w) 결정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 결정할 수 있는 것은 parameter 즉, 모수인데 이 모수를 입력으로 받으면서 실제 출력값과 model 상에서의 출력값의 차이값인 e(error)를 제곱합한 값(np.dot(e.T,e))을 출력하는 model을 생성 이 출력값의 min값을 찾는 과정을 `Optimization`이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 문제\n",
    "- 함수 f 의 값을 최소화하는 변수 x^를 찾는 것입니다.\n",
    "- x^=argmin_x f(x)-- f(x)를 최소화하게 하는 변수 x의 값 x^\n",
    "- f(x)는 objective function(J), Loss function(L), Cost function(C)라 일컫습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그리드 서치(grid search)와 수치적 최적화(numerical optimization)\n",
    "- 그리드 서치는 그냥 고등학생때 하던거라고 생각하시면 됩니다!! 하나씩 해보는 가장 간단한 방법입니다!! 굉장히 간단하지만~!, 굉장히 오래 걸려서 그냥 이런게 있구나~정도입니다!\n",
    "\n",
    "- 수치적 최적화는 `반복적 시행 착오`(trial or error)에 의해 `최적화 필요조건`을 만족하는 값을 찾는 것입니다.\n",
    "    - 현재 위치가 최적점인가?\n",
    "    - 현재 위치를 시도, 판단 후, 그 다음 시도할 위치는 어디인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 필요 조건\n",
    "- 왜 필요조건이라고 하는지는 뒤에서 얘기하겠습니다!\n",
    "- gradient(f(x))=0 -- 사실상 우리가 알고 있는 미분에 따른 기울기가 0인 포인트 x와 같은 맥락입니다. \n",
    "- 여기서 max point인가 min point인가를 알 수 있으려면?\n",
    "    - gradient(f(x),x)=0 , grandient(f(x),x,x)>0 -- min\n",
    "    - gradient(f(x),x)=0 , grandient(f(x),x,x)<0 -- max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD(Steepest Gradient Descent) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 식과 같이 현재 위치에서 gradient값을 이용해서 다음에 시도할 위치를 알아내는 방법입니다~!!\n",
    "\n",
    "X_k+1=X_k + M * gradient(f(X_k))=X_k+ M * gradient(X_k)\n",
    "\n",
    "`M` 는 스텝 사이즈로써 User가 상황에 맞춰서 지정해주어야 합니다~!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한번 해보자!\n",
    "- Scipy를 이용한 최적화\n",
    "    - Scipy의 optimize 서브 패키지에 optimize 명령어를 사용한다.\n",
    "    - Method는 인수로 설정하지 않으면, default 값으로 `BFGS` 방법이 된다.\n",
    "- x: 최적화 해\n",
    "- success: 최적화에 성공하면 True를 반환\n",
    "- status: 종료 상태. 최적화에 성공하면 0을 반환\n",
    "- message: 메세지 문자열\n",
    "- fun: x 위치에서의 함수의 값\n",
    "- jac: x 위치에서의 자코비안(그레디언트) 벡터의 값\n",
    "- hess: x 위치에서의 헤시안 행렬의 값\n",
    "- nfev: 목적함수 호출 횟수\n",
    "- njev: 자코비안 계산 횟수\n",
    "- nhev: 헤시안 계산 횟수\n",
    "- nit: x 이동 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenburg function\n",
    "def f2(x):\n",
    "    return (1 - x[0])**2 + 100.0 * (x[1] - x[0]**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p(x):\n",
    "    \"\"\"gradient of f2(x)\"\"\"\n",
    "    x1_val= 400*x[0]**3 -400*x[0]*x[1] + 2*x[0] -2\n",
    "    x2_val= -200*x[0]**2 + 200*x[1]\n",
    "    return np.array([x1_val,x2_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization이 잘 안되는 경우가 있는데 solution은 2 가지가 있다.(절대적이지 않다.)\n",
    "- 초기점 변경\n",
    "- gradient 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 2.1206175523975426e-11\n",
      " hess_inv: array([[0.49908004, 0.99785144],\n",
      "       [0.99785144, 2.00005392]])\n",
      "      jac: array([ 6.74865667e-06, -3.49484575e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 316\n",
      "      nit: 63\n",
      "     njev: 79\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.9999954 , 0.99999078])\n"
     ]
    }
   ],
   "source": [
    "a = (-3,-3)\n",
    "result = sp.optimize.minimize(f2, a)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 9.81729719189136e-14\n",
      " hess_inv: array([[0.48098005, 0.95985458],\n",
      "       [0.95985458, 1.92027267]])\n",
      "      jac: array([-8.75835558e-06,  4.59236327e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 42\n",
      "      nit: 35\n",
      "     njev: 42\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.00000021, 1.00000045])\n"
     ]
    }
   ],
   "source": [
    "a = (-2,2)\n",
    "result = sp.optimize.minimize(f2, a, jac=f2p)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 도함수를 사용한 방법\n",
    "1차 도함수인 그레디언트 정보 뿐 아니라 2차 도함수인 헤시안 행렬 정보를 사용하여 계산하는 방법이 있다.\n",
    "- CG(conjugated gradient)\n",
    "- BFGS(Broyden-Fletcher-Goldfarb-Shanno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 6.177281807662137e-16\n",
      "     jac: array([-7.45841833e-09, -2.11025224e-08])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 79\n",
      "     nit: 34\n",
      "    njev: 79\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999998, 0.99999995])\n"
     ]
    }
   ],
   "source": [
    "a=(-2,2)\n",
    "result=sp.optimize.minimize(f2,a,jac=f2p,method='CG')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 9.81729719189136e-14\n",
      " hess_inv: array([[0.48098005, 0.95985458],\n",
      "       [0.95985458, 1.92027267]])\n",
      "      jac: array([-8.75835558e-06,  4.59236327e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 42\n",
      "      nit: 35\n",
      "     njev: 42\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.00000021, 1.00000045])\n"
     ]
    }
   ],
   "source": [
    "a=(-2,2)\n",
    "result=sp.optimize.minimize(f2,a,jac=f2p,method='BFGS')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전역 최적화 문제\n",
    "- 아까 기울기 `필요` 조건이라고 언급한 적이 있다. 그 이유는 바로\n",
    "- 우리가 찾은 gradient=0 이 convex한 locally min value(local minima)일수는 있지만, globally min value(global minimum)는 아닐 수 있기 때문이다. 초기값에 따라서 not-global min value를 구할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
